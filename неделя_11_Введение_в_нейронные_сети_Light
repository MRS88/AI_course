'''Проведите серию экспериментов по перебору гиперпараметров нейронной сети, созданной на занятии.

1. Поменяйте количество нейронов в сети, используя следующие значения:

один слой 10 нейронов
один слой 100 нейронов
один слой 5000 нейронов
2. Поменяйте активационную функцию в скрытых слоях с relu на linear.
3. Поменяйте размеры batch_size:

1
10
100
Вся база (60000)
4. Запишите в таблицу получившиеся точности.
5.Напишите выводы по результатам проведенных тестов.'''


from tensorflow.keras.datasets import mnist #Библиотека с базой Mnist
from tensorflow.keras.models import Sequential # Подлючаем класс создания модели Sequential
from tensorflow.keras.layers import Dense # Подключаем класс Dense - полносвязный слой
from tensorflow.keras.optimizers import Adam # Подключаем оптимизатор Adam
from tensorflow.keras import utils #Утилиты для to_categorical
from tensorflow.keras.preprocessing import image #Для отрисовки изображения
import numpy as np # Подключаем библиотеку numpy
import matplotlib.pyplot as plt #Отрисовка изображений
from PIL import Image #Отрисовка изображений
#Отрисовывать изображения в ноутбуке, а не в консоль или файл
%matplotlib inline 

(x_train_org, y_train_org), (x_test_org, y_test_org) = mnist.load_data() #Загрузка данных Mnist
#Меняем формат входных картинок с 28х28 на 784х1
x_train = x_train_org.reshape(60000, 784)
x_test = x_test_org.reshape(10000, 784)

#Нормализуем входные картинки
x_train = x_train.astype('float32') # преобразовываем x_train в тип float
x_train = x_train / 255 # делим на 255, чтобы диапазон был от 0 до 1
x_test = x_test.astype('float32') # преобразовываем x_test в тип float
x_test = x_test / 255 # делим на 255, чтобы диапазон был от 0 до 1

# Преобразуем ответы в формат one_hot_encoding
y_train = utils.to_categorical(y_train_org, 10)
y_test = utils.to_categorical(y_test_org, 10)

model = Sequential() # Создаём сеть прямого распространения
model.add(Dense(800, input_dim=784, activation="linear"))
model.add(Dense(400, activation="linear"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"]) # Компилируем модель
print(model.summary()) #Вывод структуры модели

model.fit(x_train, y_train, batch_size=128, epochs=15, verbose=1)

n_rec = 974 #Номер тестовой цифры, которую будем распознавать
plt.imshow(Image.fromarray(x_test_org[n_rec]).convert('RGBA')) # Отрисуем картинку из тестового набора под номером n_rec
plt.show()

#Выбираем нужную картинку из тестовой выборки
x = x_test[n_rec]
x = np.expand_dims(x, axis=0)
prediction = model.predict(x) #Распознаём наш пример
print(prediction)

pred = np.argmax(prediction) # Получаем индекс самого большого элемента
print(pred)

print(y_test_org[n_rec]) # ответ для сравнения


''' Эксперимент 1. 
    Модель:
        model.add(Dense(800, input_dim=784, activation="linear"))
        model.add(Dense(10, activation="linear"))
        model.add(Dense(100, activation="linear"))
        model.add(Dense(10, activation="softmax"))
        batch_size = 1, epochs=15

    Результат:  точность = 88.9%
                время обучения = 44,5 минуты
                среднее время прохода одной эпохи = 178 сек, 3ms/step

    Сравнение нескольких картинок дает, в целом, приемлимый результат.
    Но на примере 999 с тестовой цифрой 9, сеть предсказала цифру 7
    Т.о. сеть с малым числом нейронов и малым размером батча не дает надежного результата

    ============================================================================

    Эксперимент 2.
    Модель: model.add(Dense(800, input_dim=784, activation="linear"))
            model.add(Dense(5000, activation="linear"))
            model.add(Dense(10, activation="softmax"))
            batch_size=60000

    Результат:  точность = 89.1%
                время обучения ~10 минут
                средний время прохода одной эпохи = 0s 2ms/step (при 10 минутах обучения, время прохода равно 0. Возможно вывод результата занимает гораздо больше времени)

    Сравнение порядка 10 картинок дал точность 9 из 10: по № 992 ("9") выдал "4". Что, в целом, соответствует полученной точности, но далека до надежного.
    Т.о. слой с относительно большим кол-вом нейронов в слое и батчем не годится для обучение сети.

    ============================================================================

    Эксперимент 3.
    Модель: model.add(Dense(800, input_dim=784, activation="linear"))
            model.add(Dense(100, activation="linear"))
            model.add(Dense(10, activation="softmax"))
            batch_size=100

    Результат:  точность = 92.5%
                время обучения = 2 минуты
                среднее время прохода одной эпохи = 7s 12ms/step

    Сравнение порядка 11 картинок дал точность 9 из 11: по № 985 ("2") выдал "8", № 982 ("8") выдал "3".
    При том, что в этом эксперименте точность больше, чем в предыдущей, точность при выборочной проверки оказалась ниже.
    В данном случае это можно объяснить малой выборкой.
    Т.о. выбор 100 нейронов оказался более приемлимым, т.к. показал большую точность, но не близкую к желаемой 98-99%% (мною желаемой, разумеется).
    Возможно, линейная функция активации не позволила достичь желаемого. Проверим еще одним экспериментом с 400 нейронами эту гипотезу.

    ============================================================================

    Эксперимент 4.
    Модель: model.add(Dense(800, input_dim=784, activation="linear"))
            model.add(Dense(400, activation="linear"))
            model.add(Dense(10, activation="softmax"))
            batch_size=100

    Результат:  точность = 92.5%
                время обучения = 2 минуты
                среднее время прохода одной эпохи = 10s 17ms/step

    Сравнение порядка 10 картинок дал точность 9 из 10: по № 975 ("2") выдал "3".
    Гипотеза о том, что линейная функция активации имеем "потолок" предсказания подтвердилась в этом эксперименте.
    Т.о. для обучения нейронной сети лучше использовать нелинейные функции.

